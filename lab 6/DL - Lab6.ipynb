{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qs453tkgdf3h",
    "outputId": "f4e53f6f-2c33-413e-ec98-c7fc4825ccce",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:26:02.180811Z",
     "start_time": "2025-04-09T11:25:59.048957Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from albumentations import HorizontalFlip, RandomBrightnessContrast, Resize, Compose\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hthek\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\hthek\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svh-kWQJk5Z7",
    "outputId": "2328ed8b-b5b9-41f4-f8eb-07914807048f"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Path to the ZIP file\n",
    "zip_path = \"archive.zip\"\n",
    "extract_to = \"cityscapes\"  # Folder where files will be extracted\n",
    "\n",
    "# Open the ZIP file and extract its contents\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5HChtDsKjDF8",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:28:34.609444Z",
     "start_time": "2025-04-09T11:28:34.604579Z"
    }
   },
   "source": [
    "# Dataset paths\n",
    "DATASET_DIR = r\"C:\\Users\\hthek\\PycharmProjects\\Datasets\\cityscapes\"\n",
    "\n",
    "train_images = os.path.join(DATASET_DIR, r\"train\\img\") # I Have changed the path to fit the dataset on my local device\n",
    "train_labels = os.path.join(DATASET_DIR, r\"train\\label\")\n",
    "\n",
    "val_images = os.path.join(DATASET_DIR, r\"val\\img\")\n",
    "val_labels = os.path.join(DATASET_DIR, r\"val\\label\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Jkiwlk1DnN4V",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:38.381584Z",
     "start_time": "2025-04-09T11:47:38.376438Z"
    }
   },
   "source": [
    "# Define custom dataset\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir, lbl_dir, augmentations=None):\n",
    "\n",
    "        # Path to the folder containing images & masks\n",
    "        self.img_dir = img_dir\n",
    "        self.lbl_dir = lbl_dir\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "        # Store the sorted list of filenames for images and masks to ensure they are correctly paired\n",
    "        self.images = sorted(os.listdir(img_dir))\n",
    "        self.labels = sorted(os.listdir(lbl_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        # Returns the number of images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx]) # Get image path\n",
    "        lbl_path = os.path.join(self.lbl_dir, self.labels[idx]) # Get label path\n",
    "        img = cv.imread(img_path) # Read the image\n",
    "        lbl = cv.imread(lbl_path, cv.IMREAD_GRAYSCALE) # Read the label in grayscale\n",
    "\n",
    "        # If augmentations are provided\n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=img, mask=lbl)\n",
    "            img, lbl = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "        # divide by 255 to normalize them into the range [0, 1]\n",
    "        img = img.float()/255.0\n",
    "\n",
    "        return img, lbl"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V2zjEUHOjG5j",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:40.384714Z",
     "start_time": "2025-04-09T11:47:40.368551Z"
    }
   },
   "source": [
    "# preprocessing and augmentation\n",
    "def get_augmentations(train=True):\n",
    "    if train:\n",
    "        return Compose([\n",
    "            # Resize(256, 512),  # Resize images to (256, 512)\n",
    "            HorizontalFlip(p=0.5), # Flip images horizontally with 50% probability\n",
    "            RandomBrightnessContrast(p=0.2), # Randomly change brightness/contrast with 20% probability\n",
    "            ToTensorV2() # Convert image/mask to PyTorch tensors\n",
    "        ])\n",
    "    else: # No data augmentation for validation (Only preprocessing)\n",
    "        return Compose([\n",
    "            # Resize(256, 512),\n",
    "            ToTensorV2()\n",
    "        ])"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8eQRQzD9m-L3",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:40.740046Z",
     "start_time": "2025-04-09T11:47:40.729221Z"
    }
   },
   "source": [
    "# Load datasets\n",
    "train_augmentations = get_augmentations(train=True)\n",
    "val_augmentations = get_augmentations(train=False)\n",
    "\n",
    "train_dataset = SegmentationDataset(train_images, train_labels, augmentations=train_augmentations)\n",
    "val_dataset = SegmentationDataset(val_images, val_labels, augmentations=val_augmentations)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l0TSk9NtmYdJ",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:43.049771Z",
     "start_time": "2025-04-09T11:47:43.044542Z"
    }
   },
   "source": [
    "# Define the Fully Convolutional Network (FCN)\n",
    "# VGG16 as a feature extractor (encoder) & custom decoder to reconstructs the segmentation\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        backbone = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "        # remove fully connected layers & keep only convolutional layers\n",
    "        self.encoder = backbone.features\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "          # nn.ConvTranspose2d(Input channels, Output channels, kernel_size, stride, padding, output_padding)\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64, 3, (2, 1), 1, (1, 0)),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # reduce the number of channels to match the number of segmentation classes\n",
    "            nn.Conv2d(64, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    # x = input image [batch_size, channels, height, width]\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Extracts features using VGG16 (Encoder)\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Passing features through the Decoder\n",
    "        segmentation_map = self.decoder(features)\n",
    "        segmentation_map = nn.functional.interpolate(segmentation_map, size=(96, 256), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return segmentation_map"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YpviHHKkoAQp",
    "outputId": "e4f8dbe8-700c-449b-ae68-a7d763dd5527",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:32:24.139119Z",
     "start_time": "2025-04-09T11:30:16.041068Z"
    }
   },
   "source": [
    "model = FCN(num_classes=19)\n",
    "print(model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\hthek/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [02:06<00:00, 4.37MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCN(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), output_padding=(1, 0))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(64, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95SteuPBoXN5",
    "outputId": "51899447-00dc-4617-af74-e2d8fe830e50",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:53.924575Z",
     "start_time": "2025-04-09T11:47:53.909347Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8yTFCPreoZZm",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:47:57.218465Z",
     "start_time": "2025-04-09T11:47:57.205774Z"
    }
   },
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IBMVKMQobKI",
    "outputId": "057261a7-2e13-4d5e-f757-274e596c69ab",
    "ExecuteTime": {
     "end_time": "2025-04-09T11:56:01.992745Z",
     "start_time": "2025-04-09T11:47:59.043709Z"
    }
   },
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "num_classes = 19\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for images, masks in train_loader:\n",
    "\n",
    "        # Ensurs the pixel values in masks are in the range [0, num_classes - 1].\n",
    "        masks = torch.clamp(masks, 0, num_classes-1)\n",
    "        images, mask = images.float().to(device), masks.long().to(device) # call it a mask, not label\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print('Training complete!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1345\n",
      "Epoch [2/10], Loss: 0.1206\n",
      "Epoch [3/10], Loss: 0.1180\n",
      "Epoch [4/10], Loss: 0.1158\n",
      "Epoch [5/10], Loss: 0.1137\n",
      "Epoch [6/10], Loss: 0.1110\n",
      "Epoch [7/10], Loss: 0.1086\n",
      "Epoch [8/10], Loss: 0.1054\n",
      "Epoch [9/10], Loss: 0.1027\n",
      "Epoch [10/10], Loss: 0.0998\n",
      "Training complete!\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GbNrNjzTod0v",
    "outputId": "5cfa5800-4770-4b78-ca4e-682168a242fc",
    "ExecuteTime": {
     "end_time": "2025-04-09T12:07:59.059425Z",
     "start_time": "2025-04-09T12:07:58.956166Z"
    }
   },
   "source": [
    "torch.save(model.state_dict(), 'semantic_segmentation_model.pt')\n",
    "print('Model saved!')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QU_ZZoi1xV1Z",
    "outputId": "2cdf7a53-6fef-4f8f-c23f-be65d340941f",
    "ExecuteTime": {
     "end_time": "2025-04-09T12:06:15.929564Z",
     "start_time": "2025-04-09T12:06:07.233528Z"
    }
   },
   "source": [
    "# Validation loop\n",
    "num_epochs = 1  # No need for 10 epochs , this is validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.eval()\n",
    "    correct_pixels = 0\n",
    "    total_pixels = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:  # We should use val_loader not train_loader\n",
    "            masks = torch.clamp(masks, 0, num_classes-1)\n",
    "            images, masks = images.float().to(device), masks.long().to(device)\n",
    "\n",
    "            outputs = model(images)  # Model prediction (logits)\n",
    "            predictions = torch.argmax(outputs, dim=1)  # Get class with highest probability\n",
    "\n",
    "            correct_pixels += (predictions == masks).sum().item()\n",
    "            total_pixels += masks.numel()  # Total number of pixels\n",
    "\n",
    "accuracy = correct_pixels / total_pixels * 100  # Convert to percentage\n",
    "print(f\"Final Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 97.86%\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
